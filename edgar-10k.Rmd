---
title: "PEabody Text Analysis"
author: "Tricia Jamison"
date: "June 29, 2016"
output: pdf_document
---


```{r}

Sys.getlocale()
Sys.setlocale('LC_ALL','C') 

library(data.table)
library(plyr)
library(dplyr)
library(stringr)
library(tm)
library(SnowballC)
library(ggplot2)
library(RColorBrewer)
library(scales)
s

raw.dest <- "raw-10K/"

# #############################################################################
# Downlaod the 10Ks we want

# ##
# 1) get search results page 

#cik <- "0000320193" # Apple
#cik <- "0000789019" # Microsoft
# 0001018724 Amazon
# 0000200406 J&J
# 0000040545 GE
# 0000732717
# 0000080424
# 0000310158
# 0000050863
# 0001326801 FB
# 0001288776 Google

# ciks <- c("0000320193","0000789019", "0001018724","0000200406","0000040545","0000732717","0000080424","0000310158","0000050863","0001326801","0001288776")

ciks <- read.csv("cik-codes.csv", colClasses=c("character","character"))
ciks$cik.short <- gsub("^[0]*","", ciks$cik)

all.filings <- lapply(ciks$cik, function(cik) {
  
  print(paste("Processing ", cik))
  
  index.page <- paste("https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=",cik,"&type=10-K&dateb=&owner=include&count=14",sep="")
  download.file(index.page, destfile = paste(raw.dest, cik,"-index.html", sep=""))
  
  # parse the downloaded index page to get the actual filename of each filing. Each filing has an "accension number" (acc.no)
  doc = htmlParse(paste(raw.dest, cik,"-index.html", sep=""))
  # there is one filing in each row - just grab the whole line of text
  filing.rows <- xpathSApply(doc, "//tr", xmlValue)
  # for now, don't use amended ones - just ignore 13F-HRA
  filing.rows <- grep("10-K\n", filing.rows, value=TRUE)
  
  # ##
  # 2 and 3) loop through list of filings, parse filenames, download files, return data frame with summary info
  
  # URL format for retrieving filings uses CIK code with no leading zeros
  cik <- gsub("^[0]*","", cik)
  # substr(cik,4,10)
  
  filings <- data.frame(rbindlist(lapply(filing.rows, function(x) {
    
    # messy parsing of accension number, file date from each line of text
    acc.no.dashes <- substr(unlist(strsplit(x, "Acc-no: "))[2],0,20) # accension number with dashes
    #print(acc.no.dashes)
    acc.no <- gsub("[^0-9]","",acc.no.dashes) # accension number no dashes
    #print(acc.no)
    file.date <- str_trim(unlist(strsplit(x, "\n"))[4])
    
    print(paste("Processing ", cik, " for date ", file.date))
    
    # make url to download filing
    remote.page <- paste("https://www.sec.gov/Archives/edgar/data/",cik,"/",acc.no,"/",acc.no.dashes,".txt",sep="")
    # what we save it as locally
    local.page <- paste(raw.dest, cik, "-", file.date, "-",acc.no, ".txt", sep="")
  
    if (!file.exists(local.page)) download.file(remote.page, destfile = local.page)
    
    return(data.frame(cik, acc.no, acc.no.dashes, file.date,local.page, remote.page))
  })),stringsAsFactors=FALSE)
  
  # want info as characters, not factors
  filings$local.page <- as.character(filings$local.page)
  filings$remote.page <- as.character(filings$remote.page)
  filings$cik <- as.character(filings$cik)
  filings$acc.no <- as.character(filings$acc.no)
  filings$acc.no.dashes <- as.character(filings$acc.no.dashes)
  filings$file.date <- as.Date(as.character(filings$file.date))
  
  # only keep filings after 2Q 2013 -- filings before this are in unstructured html
  filings <- filings[filings$file.date > as.Date("2007-01-01"),]
  filings
})

all.filings <- data.frame(rbindlist(all.filings))

# #############################################################################
# Do work on each filing to count years

# https://rstudio-pubs-static.s3.amazonaws.com/31867_8236987cf0a8444e962ccd2aec46d9c3.html#clustering-by-term-similarity
#in.files <- DirSource(raw.dest)$filelist
#raw.files <- list.files(raw.dest, full.names=TRUE)
#years <- as.character(c(2016:2040))

out <- data.frame(rbindlist(lapply(1:nrow(all.filings), function(x) {
  print(paste(all.filings[x,"cik"], all.filings[x,"file.date"])) 
  f <- readLines(all.filings[x, "local.page"])
  docs.start <- grep("<DOCUMENT>", f)
  docs.end <- grep("</DOCUMENT>", f)
  doc <- f[grep("<DOCUMENT>", f)[1]:grep("</DOCUMENT>", f)[1]]
  
  writeLines(doc, paste(all.filings[x, "local.page"],"-10K.txt",sep=""))
  
  words <- strsplit(doc, "\\W+")
  years <- table(grep("20[012345]+[0-9]+", unlist(words), value=TRUE))
  years <- as.data.frame.table(years, stringsAsFactors = FALSE)
  years <- years[which(nchar(years[,1]) == 4),]
  years$cik = all.filings[x,"cik"]
  years$date = all.filings[x,"file.date"]
  names(years) <- c("year", "count", "cik","filing.date")
  years
})))

out$year.mention <- paste(out$year, "-01-01", sep="")
out$year.mention <- as.Date(out$year.mention)
out$year.10k <- ifelse(as.numeric(substr(out$filing.date,6,7)) <= 7, as.numeric(substr(out$filing.date,1,4))-1, substr(out$filing.date,1,4))
out <- merge(out, ciks[,c("cik.short", "name")], by.x="cik", by.y="cik.short", all.x=TRUE, all.y=FALSE)

# graph word counts

colourCount = length(unique(out$year.10k))
getPalette = colorRampPalette(brewer.pal(9, "Set1"))

ggplot(out, aes(x=year.mention, y=count, fill=factor(year.10k))) +
#  scale_fill_brewer() + 
  scale_fill_manual(values = getPalette(colourCount)) +
  scale_x_date() +
  geom_bar(stat="identity") + facet_wrap(~ name, ncol=1)

out.pct <- data.frame(out %>%
  group_by(name, year.mention) %>%
  mutate (total.year.mention = sum(count), pct=count/total.year.mention))%>%
  arrange(name, year.mention, year.10k)

ggplot(out.pct, aes(x=year.mention, y=pct, fill=factor(year.10k))) +
#  scale_fill_brewer() + 
  scale_fill_manual(values = getPalette(colourCount)) +
  scale_x_date() +
  scale_y_continuous(label=percent) +
  geom_bar(stat="identity", position="stack") + facet_wrap(~ name, ncol=1)


# #############################################################################
# Do work on each filing to get text mentions

word <- "long-term"

out.text <- data.frame(rbindlist(lapply(1:5, function(x) {
#out.text <- data.frame(rbindlist(lapply(1:nrow(all.filings), function(x) {
  print(paste(all.filings[x,"cik"], all.filings[x,"file.date"])) 
  f <- readLines(all.filings[x, "local.page"])
  docs.start <- grep("<DOCUMENT>", f)
  docs.end <- grep("</DOCUMENT>", f)
  doc <- f[grep("<DOCUMENT>", f)[1]:grep("</DOCUMENT>", f)[1]]
  
  if (!file.exists(paste(all.filings[x, "local.page"],"-10K.txt",sep=""))) writeLines(doc, paste(all.filings[x, "local.page"],"-10K.txt",sep=""))
  
  #print(grep(word, doc, value=TRUE))
  text.mentions <- data.frame(grep(word, doc, value=TRUE,ignore.case=TRUE))
  text.mentions$cik = all.filings[x,"cik"]
  text.mentions$date = all.filings[x,"file.date"]
  text.mentions$word = word
  names(text.mentions) = c("text","cik","filing.date", "word")
  print(nrow(text.mentions))
  text.mentions[,c(2,3,4,1)]
})))
write.csv(out.text, "out.text.csv")

# #############################################################################
# Do work on whole corpus

er <- VCorpus(DirSource(raw.dest, encoding = "UTF-8"))

er2 <- tm_map(er, removePunctuation)  
#er2 <- tm_map(er2, removeNumbers)  
er2 <- tm_map(er2, tolower)  
er2 <- tm_map(er2, removeWords, stopwords("english"))   
#er3 <- tm_map(er2, stemDocument)  
er3 <- tm_map(er2, stripWhitespace)   
er4 <- tm_map(er3, PlainTextDocument)   
er4 <- er3

i <- 0
er4 <- tm_map(er3, function(x) {
  i <<- i + 1
  meta(x, tag="id", type="local") <- in.files[i]
  #meta(x, tag="id") <- in.files[i]
  print(i)
  x
})

(er[[1]])$meta
(er[[2]])$meta

dtm <- DocumentTermMatrix(er4)   
inspect(dtm[, c("gas","natgas","environment","shale", "weather","winter","summer")]) 
inspect(dtm[, grep("subst", dimnames(dtm)$Terms)]) 
inspect(dtm[, grep("inventor", dimnames(dtm)$Terms)]) 

years <- as.character(c(2015:2030))
dtm <- DocumentTermMatrix(er)   
inspect(dtm[,years])

f <- readLines(in.files[1])
grep("201[0-9]+", f, value=TRUE)

```


